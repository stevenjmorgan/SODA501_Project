{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script, written in Python 2.7, downloads webpages that hold press/news releases from 28 state parties with BeautifulSoup and Selenium. Releases from some states were collected manually, since there were so few releases (i.e. Indiana GOP has posted 3 press releases). \n",
    "\n",
    "The files are written in UTF-8 encoding to .txt files. It should be noted that running these scripts at different times will produce slightly different data. This occurs because the script specifies pages to scrape from, and which pages press releases are located will change as more press releases are added. Code to write to specific directories (i.e. write files from a state to folder just for that state) is currently commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Steven Morgan\n",
    "### Scraping Press Releases from State Republican Party Websites\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2, re, requests, bs4, os\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maine Releases\n",
    "\n",
    "Create empty list to store press release urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through pages of press release links.\n",
    "The url for each page follows a set pattern (http://www.mainegop.com/news/previous/3) therefore each iteration changes the page of press release links.\n",
    "Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Python27\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-on-dem-gubernatorial-candidates-push-for-another-tax-hike\n",
      "//www.mainegop.com/news/maine-gop-statement-on-the-the-loss-of-somerset-county-sheriffs-deputy-eugene-cole\n",
      "//www.mainegop.com/news/statement-on-speaker-gideons-outrageous-comments\n",
      "//www.mainegop.com/news/maine-gop-statement-on-the-passing-of-barbara-bush\n",
      "//www.mainegop.com/news/maine-gop-chair-on-house-dems-defending-female-genital-mutilation\n",
      "//www.mainegop.com/news/maine-gop-statement-on-judges-rcv-ruling\n",
      "//www.mainegop.com/news/maine-gop-chair-to-seth-carey-drop-out-of-your-race\n",
      "//www.mainegop.com/news/demi-to-sara-what-are-you-afraid-of\n",
      "//www.mainegop.com/news/attorney-general-janet-mills-tears-down-ethical-wall\n",
      "//www.mainegop.com/news/demi-kouzounas-statement-on-the-passing-of-don-collins\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-state-of-the-state-address\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-state-of-the-union-address\n",
      "//www.mainegop.com/news/maine-gop-statement-explains-total-compliance-in-debunking-maine-dems-meritless-ethics-complaint\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-maine-dem-ethics-complaint\n",
      "//www.mainegop.com/news/maine-executive-director-statement-on-democrats-government-shutdown\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-decision-to-give-states-more-flexibility-with-administering-medicaid\n",
      "//www.mainegop.com/news/governor-lepage-hails-trump-administration-decision-on-medicaid\n",
      "//www.mainegop.com/news/rep-poliquin-statement-on-major-welfare-reform\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-dems-threatening-medicaid-lawsuit\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-house-passage-of-tax-relief\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-shane-bouchard-victory-in-lewiston-and-sweeping-maine-mayoral-races-in-2017\n",
      "//www.mainegop.com/news/maine-gop-executive-director-calls-on-maine-democrats-to-take-a-position-on-legislation-to-protect-concealed-carry-rights\n",
      "//www.mainegop.com/news/lewiston-mayor-appalled-by-ben-chin-emails\n",
      "//www.mainegop.com/news/statements-on-tax-reform-vote\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-appalling-report-about-ben-chin-campaign\n",
      "//www.mainegop.com/news/levesque-thanks-the-people-of-auburn-for-putting-their-faith-in-him\n",
      "//www.mainegop.com/news/democrats-are-putting-politics-ahead-of-middle-class-tax-relief\n",
      "//www.mainegop.com/news/maine-dem-chair-says-he-would-raid-rainy-day-fund-to-expand-welfare\n",
      "//www.mainegop.com/news/house-dem-leader-jared-golden-has-no-idea-how-to-pay-for-maine-medicaid-expansion\n",
      "//www.mainegop.com/news/maine-gop-chair-congratulates-jason-levesque-for-winning-election-as-auburn-mayor\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-chair-congratulates-marston-lovell-for-winning-sacos-mayoral-election\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-lewiston-mayoral-election-results\n",
      "//www.mainegop.com/news/maine-gop-chair-congratulates-nick-isgro-for-winning-re-election-as-waterville-mayor\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-the-election-of-rick-mason-to-the-maine-legislature\n",
      "//www.mainegop.com/news/maine-gop-chair-statement-on-welfare-expansion-referendum-passage-under-questionable-circumstances\n",
      "//www.mainegop.com/news/maine-gop-chair-kouzounas-on-the-passing-of-former-maine-gop-chair-kathy-watson\n",
      "//www.mainegop.com/news/maine-gop-chair-applauds-middle-class-tax-reform-framework\n",
      "//www.mainegop.com/news/maine-gops-jason-savage-calls-on-maine-dem-chair-to-denounce-dnc-corruption\n",
      "//www.mainegop.com/news/ag-mills-must-investigate-illegal-signature-gathering-for-question-2\n",
      "//www.mainegop.com/news/breaking-maine-gop-exposes-illegal-signature-gathering-by-yes-on-2-supporters\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-executive-director-statement-on-presidents-action-on-opioid-epidemic\n",
      "//www.mainegop.com/news/maine-gop-calls-out-false-claim-from-pro-welfare-expansion-group\n",
      "//www.mainegop.com/news/maine-gop-executive-director-statement-on-king-and-pingree-hiding-their-obstructionism-with-a-national-democrat-talking-point\n",
      "//www.mainegop.com/news/maine-gop-announces-hire-of-political-and-communications-director\n",
      "//www.mainegop.com/news/exposed-documents-show-lucas-st-clair-still-registered-in-portland-came-back-to-maine-by-way-of-seattle-wa-just-a-few-short-years-ago\n",
      "//www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-applauds-federal-tax-reform-proposal\n",
      "//www.mainegop.com/news/maine-gop-appreciates-governor-lepage-effort-to-protect-maine-american-people\n",
      "//www.mainegop.com/news/chair-kouzounas-issues-statement-on-daca\n",
      "//www.mainegop.com/news/maine-gop-continues-to-staff-up-for-2018-cycle\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/chair-kouzounas-issues-statement-on-mark-eves-comments\n",
      "//www.mainegop.com/news/chair-kouzounas-issues-statement-on-tax-reform\n",
      "//www.mainegop.com/news/maine-gop-statement-on-golden-announcement\n",
      "//www.mainegop.com/news/maine-gop-chair-kouzounas-issues-statement-on-charlottesville\n",
      "//www.mainegop.com/news/conrad-tavarez-tapped-as-maine-gop-data-director\n",
      "//www.mainegop.com/news/ethically-challenged-diane-russell-announces-gubernatorial-campaign\n",
      "//www.mainegop.com/news/radical-ties-of-maine-dem-lawmaker-who-threatened-president-trump\n",
      "//www.mainegop.com/news/democratic-lawmaker-scott-hamanns-radical-past\n",
      "//www.mainegop.com/news/maine-gop-congratulates-legislature-and-governor-lepage-on-budget-agreement\n",
      "//www.mainegop.com/news/64-of-maine-house-democrats-threatened-shutdown-state-government\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/chair-kouzounas-on-comey-hearing\n",
      "//www.mainegop.com/news/senator-king-and-congresswoman-pingree-draft-plan-to-fix-obamacare\n",
      "//www.mainegop.com/news/maine-gop-statement-on-cost-of-obamacare\n",
      "//www.mainegop.com/news/maine-gop-applauds-ruling-on-ranked-choice-voting\n",
      "//www.mainegop.com/news/maine-gop-releases-video-exposing-democrat-opportunity-agenda-as-a-massive-spending-tax-increase-agenda\n",
      "//www.mainegop.com/news/maine-gop-statement-on-maines-lowest-unemployment-rate-since-1976\n",
      "//www.mainegop.com/news/maine-gop-statement-on-sanders-and-perez-event-in-portland\n",
      "//www.mainegop.com/news/maine-gop-statement-on-judge-gorsuch-confirmation\n",
      "//www.mainegop.com/news/maine-gop-issues-statement-on-senator-angus-kings-filibuster-support\n",
      "//www.mainegop.com/news/maine-gop-issues-statement-on-senator-angus-kings-scotus-decision\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-thanks-senator-susan-collins-for-support-of-judge-neil-gorsuch-against-fillibuster-on-senate-floor\n",
      "//www.mainegop.com/news/maine-house-corruption-problems-grow-former-house-taxation-committee-chair-faces-investigation-for-illegal-lobbying-for-union-in-maine-state-house\n",
      "//www.mainegop.com/news/gov-lepage-joined-fox-news-neil-cavuto\n",
      "//www.mainegop.com/news/insane-sen-ben-chipman-d-portland-might-support-slavery-if-passed-by-referendum\n",
      "//www.mainegop.com/news/maine-gop-applauds-congressman-poliquin-and-governor-lepage-on-aca-replacement-work\n",
      "//www.mainegop.com/news/despite-having-two-options-to-fund-support-for-mainers-with-intellectual-disabilities-maines-health-human-services-democrats-are-about-to-leave-them-out-in-the-cold\n",
      "//www.mainegop.com/news/maine-gop-chair-applauds-androscoggin-county-resolution-on-house-ethics\n",
      "//www.mainegop.com/news/dems-insult-maine-workers-say-maine-should-double-down-on-welfare-for-non-citizens\n",
      "//www.mainegop.com/news/march-09th-2017\n",
      "//www.mainegop.com/news/maine-gop-files-preservation-of-records-notice-to-rep-tipping\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-chairman-to-senator-king-come-to-congressional-district-2\n",
      "//www.mainegop.com/news/maine-gop-files-preservation-of-records-notices-with-speaker-gideon-rep-tipping\n",
      "//www.mainegop.com/news/listen-jason-savages-interview-on-howie-carr-weekend-wrap-up-with-jack-heath\n",
      "//www.mainegop.com/news/bdn-refused-to-publish-this-op-ed-from-rep-karl-ward-on-tipping-scandal\n",
      "//www.mainegop.com/news/document-maine-dem-state-rep-faces-10k-ethics-fine\n",
      "//www.mainegop.com/news/watch-gov-lepage-on-fox-and-friends\n",
      "//www.mainegop.com/news/audio-rep-tipping-uses-committee-position-to-lobby\n",
      "//www.mainegop.com/news/maine-gop-files-foaa-requests-with-speaker-gideon-rep-tipping\n",
      "//www.mainegop.com/news/maine-gop-joins-36-state-letter-of-support-for-judge-neil-gorsuch\n",
      "//www.mainegop.com/news/exposing-major-conflict-of-interest\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-thanks-usm-president-glenn-cummings-for-promise-to-protect-free-speech-on-campus\n",
      "//www.mainegop.com/news/maine-gop-chair-demi-kouzounas-statement-on-governor-lepages-state-of-state-address\n",
      "//www.mainegop.com/news/maine-gop-announces-foaa-of-portland-superintendent\n",
      "//www.mainegop.com/news/maine-gop-statement-on-scotus\n",
      "//www.mainegop.com/news/maine-gop-announces-leadership-election-results7489855\n",
      "//www.mainegop.com/news/maine-dems-issue-false-attacks-on-women\n",
      "//www.mainegop.com/news/obamacare-op-ed-the-portland-press-herald-wont-publish\n",
      "//www.mainegop.com/news/senator-king-likens-obamacare-repeal-to-blowing-up-an-apartment-building\n",
      "//www.mainegop.com/news/rep-poliquin-to-vote-for-health-insurance-relief-for-thousands-of-maine-families\n",
      "//www.mainegop.com/news/sen-collins-is-under-attack-on-sessions\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-chairman-richard-bennett-issues-statement-following-todays-legislative-council-meeting\n",
      "//www.mainegop.com/news/statement-from-maine-gop-chairman-rick-bennett-on-federal-court-dismissal-of-speaker-eves-political-lawsuit\n",
      "//www.mainegop.com/news/maine-gop-chairman-rick-bennetts-statement-regarding-maine-election-results\n",
      "//www.mainegop.com/news/alert\n",
      "//www.mainegop.com/news/maine-gop-election-day-legal-contacts\n",
      "//www.mainegop.com/news/maine-gop-final-statement-on-bates-flyers\n",
      "//www.mainegop.com/news/emily-cain-calls-hillary-clinton-person-of-good-character\n",
      "//www.mainegop.com/news/michaud-allies-slam-lepage-for-dem-bill\n",
      "//www.mainegop.com/news/maine-gop-calls-on-emily-cain-to-return-250k-wells-fargo-support-from-hillary-clinton\n",
      "//www.mainegop.com/news/video-emily-cain-admits-her-ads-are-lies\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/press-release-maine-democrats-play-cynical-hypocritical-game-with-voters-media\n",
      "//www.mainegop.com/news/emily-cain-admits-shes-only-had-one-job-politician\n",
      "//www.mainegop.com/news/commissioner-walt-whitcomb-to-serve-on-donald-trumps-agricultural-advisory-committee\n",
      "//www.mainegop.com/news/maine-gop-issues-statement-on-immigration-reform\n",
      "//www.mainegop.com/news/maine-dems-tied-to-dnc-corruption\n",
      "//www.mainegop.com/news/emily-cain-voted-to-give-drivers-licenses-to-illegal-immigrants\n",
      "//www.mainegop.com/news/a-statement-from-chairman-richard-bennett-on-the-pulse-nightclub-shooting\n",
      "//www.mainegop.com/news/cd-1-candidate-forum-tuesday-june-7th\n",
      "//www.mainegop.com/news/maine-federation-of-republican-women-turns-70\n",
      "//www.mainegop.com/news/emily-cain-is-pro-national-park-anti-maine\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/watch-chairman-rick-bennetts-state-of-the-party-speech\n",
      "//www.mainegop.com/news/just-in-maine-gop-names-page-for-national-convention\n",
      "//www.mainegop.com/news/breaking-house-speaker-mark-eves-lawsuit-against-governor-lepage-dropped\n",
      "//www.mainegop.com/news/warning-maine-gop-issues-alert-to-maine-press-conservative-activists-about-dark-money-operative-pretending-to-be-member-of-press\n",
      "//www.mainegop.com/news/maine-gop-convention-delegate-alternate-election-results\n",
      "//www.mainegop.com/news/attention-maine-republican-party-releases-additional-special-public-tickets-for-maine-state-gop-convention-and-events-this-week\n",
      "//www.mainegop.com/news/sununu-to-keynote-maine-gop-convention-for-governor-john-kasich\n",
      "//www.mainegop.com/news/statements-of-2016-maine-gop-national-delegate-candidates\n",
      "//www.mainegop.com/news/maine-gop-announces-carly-fiorina-will-be-speaker-at-gop-convention\n",
      "//www.mainegop.com/news/breaking-new-document-further-exposes-emily-cains-hypocrisy-on-wealthy-donors-and-dark-money-groups\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-announces-dr-ben-carson-as-banquet-keynote-speaker\n",
      "//www.mainegop.com/news/just-in-record-number-of-maine-republicans-running-to-secure-national-delegate-position\n",
      "//www.mainegop.com/news/press-release-emily-cain-out-of-state-and-out-of-touch\n",
      "//www.mainegop.com/news/senate-republicans-announce-candidates-in-all-35-districts-for-2016\n",
      "//www.mainegop.com/news/house-republicans-field-149-candidates-for-november-election\n",
      "//www.mainegop.com/news/final-maine-gop-caucus-results\n",
      "//www.mainegop.com/news/maine-gop-announces-caucus-sites-schedules\n",
      "//www.mainegop.com/news/democrat-leader-mccabe-responds-to-governor-lepage-pos\n",
      "//www.mainegop.com/news/statement-from-chairman-bennett-on-governor-lepages-state-of-the-state-address\n",
      "//www.mainegop.com/news/irresponsible-emily-how-emily-cains-past-rhetoric-about-iran-deal-proves-her-position-was-irresponsible\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/chairmans-statement-on-passing-of-senator-lois-snowe-mello\n",
      "//www.mainegop.com/news/republican-leadership-matters-combating-maines-drug-crisis\n",
      "//www.mainegop.com/news/read-congressman-poliquins-op-ed\n",
      "//www.mainegop.com/news/press-release-shifty-and-shady\n",
      "//www.mainegop.com/news/report-maine-dems-funneling-clinton-cash\n",
      "//www.mainegop.com/news/recap-maine-democrats-re-gift-same-old-failed-rhetoric-and-candidates-in-campaign-kick-off\n",
      "//www.mainegop.com/news/statements-from-senators-thibodeau-and-mason-on-democrats-campaign-announcement\n",
      "//www.mainegop.com/news/senator-marco-rubio-qualifies-for-maine-caucus-ballot\n",
      "//www.mainegop.com/news/maine-gop-announces-2016-presidential-nominating-caucus-candidates\n",
      "//www.mainegop.com/news/maine-gop-congratulates-mayor-macdonald-on-re-election-as-lewiston-mayor\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/governor-chris-christie-qualifies-for-maine-caucus-ballot5137015\n",
      "//www.mainegop.com/news/carly-fiorina-qualifies-for-maine-nominating-caucus-ballot\n",
      "//www.mainegop.com/news/dr-ben-carson-qualifies-for-maine-caucus-ballot\n",
      "//www.mainegop.com/news/maine-gop-calls-out-alfond-press-herald-for-comments-on-syrian-refugee-situation\n",
      "//www.mainegop.com/news/speaker-mark-eves-used-influence-as-public-official-in-support-of-portland-mayor-withholding-mandatory-charter-school-funding\n",
      "//www.mainegop.com/news/senator-ted-cruz-qualifies-for-maine-caucus-ballot\n",
      "//www.mainegop.com/news/republican-special-election-wins-alter-political-landscape-in-augusta\n",
      "//www.mainegop.com/news/she-leads-fall-conference-photos\n",
      "//www.mainegop.com/news/donald-j-trump-qualifies-for-maine-presidential-nominating-caucus\n",
      "//www.mainegop.com/news/governor-jeb-bush-officially-qualifies-for-maine-gop-caucus\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-statement-on-tonights-debate\n",
      "//www.mainegop.com/news/ohio-governor-john-kasich-first-candidate-to-file-for-maine-caucus\n",
      "//www.mainegop.com/news/poliquin-votes-to-increase-womens-health-care-funding-for-the-second-district\n",
      "//www.mainegop.com/news/maine-gop-announces-presidential-nominating-changes\n",
      "//www.mainegop.com/news/maine-gop-calls-on-cain-baldacci-to-be-transparent-with-maine-people-on-iran-positions\n",
      "//www.mainegop.com/news/lester-ordway-chosen-to-seek-open-standish-house-seat\n",
      "//www.mainegop.com/news/press-release-gop-exposes-pingrees-priorities\n",
      "//www.mainegop.com/news/come-to-september-18th-gen207-presentsask-me-almost-anything\n",
      "//www.mainegop.com/news/memo-pingrees-hypocrisy-on-money-in-politics-transparency\n",
      "//www.mainegop.com/news/matt-harrington-selected-as-republican-candidate-for-hd-19\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/maine-gop-state-committee-unanimously-authorizes-party-to-act-on-referenda\n",
      "//www.mainegop.com/news/how-democrat-jeff-mccabes-false-claim-became-news\n",
      "//www.mainegop.com/news/chairman-bennetts-statement-on-the-passing-of-rep-noon\n",
      "//www.mainegop.com/news/speaker-eves-defied-ethics-commission\n",
      "//www.mainegop.com/news/seven-questions-for-mark-eves\n",
      "//www.mainegop.com/news/maine-gop-statement-on-good-will-hinckley-controversy\n",
      "//www.mainegop.com/news/congratulations-to-chairman-bennett\n",
      "//www.mainegop.com/news/visiting-washington-county\n",
      "//www.mainegop.com/news/democrats-want-to-shut-it-down\n",
      "//www.mainegop.com/news/maine-gop-statement-on-democrats-vote-denying-maine-people-voice-on-state-income-tax\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "//www.mainegop.com/news/dems-trying-to-revive-obamacares-welfare-expansion\n",
      "//www.mainegop.com/news/gen207-releases-inaugural-40-under-40-award-list\n",
      "//www.mainegop.com/news/debunking-myths-about-eliminating-the-income-tax\n",
      "//www.mainegop.com/news/ccrc-chairman-eric-lusk-calls-on-councilor-emery-to-resign\n",
      "//www.mainegop.com/news/watch-congressman-bruce-poliquins-100-days-update\n",
      "//www.mainegop.com/news/willette-appointed-to-rnc-committee-on-arrangements\n",
      "//www.mainegop.com/news/maine-gop-chairman-clinton-scandals\n",
      "//www.mainegop.com/news/maine-gop-statement-on-poliquin-fundraising-record\n",
      "//www.mainegop.com/news/maine-gop-announces-leadership-election-results\n",
      "//www.mainegop.com/news/congressman-poliquin-reacts-to-obamas-state-of-the-union\n",
      "http://www.mainegop.com/news/maine-gop-publishes-draft-2018-party-platform-provides-amendment-information\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20): #20 \n",
    "\tx = \"http://www.mainegop.com/news/previous/\" + str(i) #+ \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('www.mainegop.com/news/', str(link)) and not re.search('#comments', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through de-duped list of press release pages. Store release title, date, and contents. Write to .txt files. Option to store in a specific directory is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/8/2016\n",
      "4/12/2018\n",
      "3/7/2017\n",
      "6/29/2015\n",
      "9/29/2017\n",
      "10/17/2016\n",
      "12/1/2017\n",
      "1/25/2016\n",
      "12/1/2015\n",
      "6/7/2016\n",
      "1/24/2017\n",
      "6/16/2016\n",
      "2/19/2016\n",
      "12/19/2017\n",
      "2/3/2017\n",
      "11/21/2017\n",
      "11/8/2017\n",
      "6/8/2017\n",
      "11/3/2017\n",
      "11/23/2016\n",
      "1/20/2015\n",
      "6/5/2017\n",
      "11/1/2017\n",
      "1/30/2017\n",
      "1/11/2018\n",
      "9/14/2015\n",
      "1/25/2018\n",
      "7/15/2015\n",
      "5/18/2016\n",
      "3/7/2017\n",
      "11/14/2017\n",
      "2/27/2017\n",
      "4/17/2017\n",
      "11/3/2017\n",
      "11/8/2017\n",
      "4/4/2018\n",
      "1/4/2016\n",
      "3/24/2017\n",
      "10/14/2015\n",
      "3/6/2017\n",
      "10/17/2017\n",
      "4/13/2016\n",
      "7/19/2017\n",
      "2/22/2017\n",
      "11/8/2017\n",
      "7/19/2017\n",
      "2/28/2017\n",
      "6/24/2015\n",
      "1/8/2016\n",
      "4/21/2017\n",
      "3/27/2017\n",
      "11/8/2016\n",
      "7/25/2016\n",
      "5/29/2015\n",
      "4/12/2016\n",
      "6/29/2015\n",
      "3/14/2017\n",
      "2/18/2015\n",
      "8/28/2015\n",
      "4/18/2016\n",
      "4/27/2017\n",
      "5/2/2016\n",
      "10/3/2016\n",
      "10/13/2016\n",
      "4/2/2018\n",
      "1/26/2017\n",
      "11/13/2017\n",
      "5/11/2016\n",
      "2/8/2016\n",
      "8/31/2017\n",
      "1/31/2018\n",
      "2/10/2016\n",
      "12/12/2017\n",
      "6/22/2015\n",
      "3/21/2017\n",
      "10/27/2017\n",
      "1/5/2016\n",
      "3/28/2017\n",
      "4/19/2016\n",
      "7/17/2015\n",
      "2/8/2017\n",
      "3/17/2016\n",
      "5/18/2016\n",
      "3/30/2018\n",
      "9/17/2015\n",
      "5/5/2015\n",
      "10/2/2017\n",
      "3/6/2017\n",
      "4/15/2015\n",
      "4/4/2017\n",
      "10/28/2016\n",
      "2/13/2018\n",
      "4/28/2016\n",
      "11/8/2017\n",
      "5/24/2017\n",
      "11/7/2016\n",
      "4/20/2016\n",
      "3/7/2017\n",
      "9/19/2015\n",
      "9/29/2017\n",
      "10/20/2015\n",
      "3/15/2017\n",
      "5/4/2016\n",
      "8/29/2015\n",
      "8/31/2015\n",
      "4/2/2018\n",
      "4/18/2016\n",
      "9/5/2017\n",
      "1/14/2016\n",
      "1/11/2018\n",
      "11/12/2015\n",
      "11/3/2017\n",
      "3/21/2017\n",
      "6/8/2015\n",
      "3/6/2016\n",
      "4/14/2015\n",
      "4/6/2017\n",
      "2/8/2017\n",
      "12/3/2015\n",
      "8/14/2017\n",
      "1/25/2016\n",
      "12/7/2017\n",
      "12/16/2015\n",
      "1/8/2018\n",
      "4/12/2016\n",
      "11/8/2017\n",
      "10/13/2017\n",
      "12/9/2015\n",
      "12/15/2015\n",
      "4/24/2015\n",
      "2/17/2017\n",
      "3/9/2017\n",
      "4/7/2017\n",
      "9/1/2017\n",
      "3/11/2018\n",
      "6/16/2017\n",
      "1/13/2017\n",
      "11/28/2017\n",
      "9/19/2015\n",
      "8/10/2017\n",
      "9/26/2017\n",
      "9/28/2017\n",
      "5/23/2017\n",
      "3/16/2016\n",
      "8/17/2016\n",
      "4/30/2018\n",
      "1/20/2016\n",
      "4/29/2015\n",
      "10/23/2016\n",
      "1/26/2018\n",
      "4/17/2018\n",
      "7/20/2015\n",
      "11/6/2017\n",
      "11/10/2015\n",
      "3/28/2017\n",
      "7/4/2017\n",
      "6/12/2015\n",
      "10/22/2015\n",
      "9/15/2016\n",
      "11/19/2015\n",
      "10/23/2015\n",
      "11/4/2015\n",
      "12/1/2016\n",
      "9/5/2017\n",
      "1/10/2017\n",
      "4/18/2016\n",
      "12/7/2017\n",
      "8/24/2017\n",
      "4/19/2018\n",
      "5/11/2015\n",
      "8/14/2017\n",
      "5/13/2016\n",
      "7/11/2016\n",
      "4/25/2018\n",
      "1/11/2018\n",
      "1/12/2017\n",
      "3/1/2017\n",
      "2/14/2017\n",
      "12/8/2015\n",
      "8/28/2015\n",
      "4/23/2015\n",
      "1/20/2018\n",
      "1/31/2017\n",
      "8/19/2016\n",
      "10/6/2017\n",
      "11/17/2016\n",
      "1/16/2016\n",
      "11/9/2017\n",
      "6/26/2015\n",
      "9/25/2015\n",
      "11/8/2017\n"
     ]
    }
   ],
   "source": [
    "#os.makedirs('maineGOP_press_releases')\n",
    "for link in links:\n",
    "\tif not re.search('http:', str(link)):\n",
    "\t\tres = requests.get(str('http:' + str(link)))\n",
    "\telse:\n",
    "\t\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('p', class_ = 'blog-date')\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.select('h2')\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\tprint re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tmaine = open('MEgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#maine = open('MEgop_' + link[29:-1] + '.txt', 'w')\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\t#maine.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\t#maine.write('\\n')\n",
    "\t\t\t\tmaine.write(re.split('</', re.split('text\">', str(pub))[1])[0].strip())\n",
    "\t\t\t\tmaine.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tmaine.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tmaine.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tmaine.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 32): #32\n",
    "\tx = \"https://algop.org/category/press-releases/page/\" + str(i) + \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', class_ = 'button'):  #soup.find_all('a', href=True)\n",
    "\t\tif re.search('https://algop.org/', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('alabamaGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/alaskaGOP_press_releases')\n",
    "for link in links:\n",
    "\tres = ''\n",
    "\twhile res == '':\n",
    "\t\ttry:\n",
    "\t\t\tres = requests.get(link)\n",
    "\t\texcept:\n",
    "\t\t\tbreak\n",
    "\t#res = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('time', attrs={'class': 'entry-date published updated'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\tprint str(pub)\n",
    "\t#print str(title)\n",
    "\t#alabama = open(re.sub('/','_',('ALgop_' + link[18:-1] + '.txt')), 'w')\n",
    "\talabama = open('ALgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint str(pub)\n",
    "\t\t\t\tprint str(title)\n",
    "\t\t\t\talabama.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\talabama.write('\\n')\n",
    "\t\t\t\talabama.write(re.split('T', re.split('datetime=\"', str(pub))[1])[0])\n",
    "\t\t\t\talabama.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\talabama.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\talabama.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\talabama.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alaska releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for i in range(1, 107): \n",
    "\tx = \"http://www.alaskagop.net/news/page/\" + str(i) + \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', class_ = 'moreLink'): #soup.find_all('a', href=True, attrs={'class': 'moreLink'})\n",
    "\t\tif re.search('http://www.alaskagop.net', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('alaskaGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/alaskaGOP_press_releases')\n",
    "for link in links:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\t#pub = noStarchSoup.find('i', attrs={'class': 'fa fa-clock-o'})  \n",
    "\tpub = noStarchSoup.select('span')\n",
    "\ttitle = noStarchSoup.select('h5')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print str(title)\n",
    "\t#alaska = open(re.sub('/','_',('AKgop_' + link[25:-1] + '.txt')), 'w')\n",
    "\talaska = open('AKgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint str(pub)\n",
    "\t\t\t\tprint str(title)\n",
    "\t\t\t\talaska.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\talaska.write('\\n')\n",
    "\t\t\t\talaska.write(re.split('</', re.split('</i> ', str(pub))[1])[0])\n",
    "\t\t\t\talaska.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\talaska.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\talaska.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\talaska.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iowa releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.iowagop.org/category/news/releases/page/2/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 24): #24 \n",
    "\tx = \"https://www.iowagop.org/category/news/releases/page/\" + str(i) + \"/\"\n",
    "\t#resp = urllib2.urlopen(x)\n",
    "\ttry:\n",
    "\t\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('www.iowagop.org/20', str(link)) and not re.search('#comments', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('IA_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('ul', class_ = 'meta')\n",
    "\tprint pub\n",
    "\t#print re.split('</li> ', re.split('</a></li>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tiowa = open('IAgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tiowa.write(re.split('</', re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tiowa.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tiowa.write(re.split('<li>', re.split('</li> ', re.split('</a></li>', str(pub))[1])[0].strip())[1])\n",
    "\t\t\t\t\tiowa.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tiowa.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tiowa.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tiowa.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mississippi releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://msgop.org/news/page/2/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 37): #37\n",
    "\tx = \"http://msgop.org/news/page/\" + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\t#try:\n",
    "\t#\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t#\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\t#except:\n",
    "\t#\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('http://msgop.org/', str(link)) and len(str(link['href'])) > 25 and not re.search('tag', str(link)) and not re.search('category', str(link)) and not re.search('msgop.org/news', str(link)) and not re.search('msgop.org/officials|about-us|all-events|sgop.org/volunteer|get-involved', str(link)): #and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\t\t#if not re.search('http', str(link)) and len(str(link['href'])) > 17 and not re.search('twitter', str(link)):\n",
    "\t\t\t#print link['href']\n",
    "\t\t\t#links = links + ['https://southcarolina.gop/orp-press-releases' + str(i) + str(link['href'])]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('MS_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/MS_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#print pElems\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('div', class_ = 'postDate')\n",
    "\t#pub = noStarchSoup.find('span', class_ = 'aQJ') # date\n",
    "\tprint pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h2')#, class_ = 'heading')\n",
    "\n",
    "\tmississippi = open('MSgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tmississippi.write(re.split(\"<\", re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tmississippi.write('\\n')\n",
    "\t\t\t\tmississippi.write(re.split('</', re.split('>', str(pub))[1])[0].strip())\n",
    "\t\t\t\tmississippi.write('\\n')\n",
    "\t\t\t\tmississippi.write('\\n')\n",
    "\t\t\t\t#try:\n",
    "\t\t\t\t#\tmississippi.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t#\tmississippi.write('\\n')\n",
    "\t\t\t\t#except:\n",
    "\t\t\t\t#\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tmississippi.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tmississippi.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tmississippi.write(\"\\n\")\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "South Dakota releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://southdakotagop.com/stay-informed/press-releases/P0)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 13): #13\n",
    "\tx = \"http://southdakotagop.com/stay-informed/press-releases/P0\" + str((i - 1) * 6) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\t#try:\n",
    "\t#\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t#\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\t#except:\n",
    "\t#\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('http://southdakotagop.com/stay-informed/press-releases/', str(link)) and len(str(link['href'])) > 65: #and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\t\t#if not re.search('http', str(link)) and len(str(link['href'])) > 17 and not re.search('twitter', str(link)):\n",
    "\t\t\t#print link['href']\n",
    "\t\t\t#links = links + ['https://southcarolina.gop/orp-press-releases' + str(i) + str(link['href'])]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('SD_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/SD_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#print pElems\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('h3', class_ = 'item-date')\n",
    "\t#pub = noStarchSoup.find('span', class_ = 'aQJ') # date\n",
    "\tprint pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h2')#, class_ = 'heading')\n",
    "\t\n",
    "\tsouthdakota = open('SDgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tsouthdakota.write(re.split(\"<\", re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tsouthdakota.write('\\n')\n",
    "\t\t\t\tsouthdakota.write(re.split('</', re.split('>', str(pub))[1])[0].strip())\n",
    "\t\t\t\tsouthdakota.write('\\n')\n",
    "\t\t\t\tsouthdakota.write('\\n')\n",
    "\t\t\t\t#try:\n",
    "\t\t\t\t#\tsouthdakota.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t#\tsouthdakota.write('\\n')\n",
    "\t\t\t\t#except:\n",
    "\t\t\t\t#\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tsouthdakota.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tsouthdakota.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tsouthdakota.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "South Carolina releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.sc.gop/blog/page/3/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 40): #94\n",
    "\tx = \"https://www.sc.gop/blog/page/\" + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('https://www.sc.gop/20', str(link)): #and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\t\t#if not re.search('http', str(link)) and len(str(link['href'])) > 17 and not re.search('twitter', str(link)):\n",
    "\t\t\t#print link['href']\n",
    "\t\t\t#links = links + ['https://southcarolina.gop/orp-press-releases' + str(i) + str(link['href'])]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('SC_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/SC_gop')\n",
    "for link in links:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#print pElems\n",
    "\tpub = noStarchSoup.find('li', class_ = 'meta-date')\n",
    "\ttitle = noStarchSoup.find('h1')#, class_ = 'heading')\n",
    "\n",
    "\t\n",
    "\tsouthcarolina = open('SCgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tsouthcarolina.write(re.split(\"<\", re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tsouthcarolina.write('\\n')\n",
    "\t\t\t\tsouthcarolina.write(re.split('</', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\tsouthcarolina.write('\\n')\n",
    "\t\t\t\tsouthcarolina.write('\\n')\n",
    "\t\t\t\t#try:\n",
    "\t\t\t\t#\tsouthcarolina.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t#\tsouthcarolina.write('\\n')\n",
    "\t\t\t\t#except:\n",
    "\t\t\t\t#\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tsouthcarolina.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('SOUTH CAROLINA REPUBLICAN PARTY FOR IMMEDIATE RELEASE', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tsouthcarolina.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tsouthcarolina.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oregon releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://oregon.gop/orp-press-releases)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 2): #2\n",
    "\tx = \"https://oregon.gop/orp-press-releases\" # + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\t#try:\n",
    "\t#\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t#\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\t#except:\n",
    "\t#\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\t#if re.search('http://nh.gop', str(link)) and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + [link['href']]\n",
    "\t\tif not re.search('http', str(link)) and len(str(link['href'])) > 17 and not re.search('twitter', str(link)):\n",
    "\t\t\t#print link['href']\n",
    "\t\t\tlinks = links + ['https://oregon.gop/orp-press-releases' + str(i) + str(link['href'])]\n",
    "\n",
    "links = links[17:]\n",
    "links = list(set(links))\n",
    "#print links\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('OR_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/OR_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\tprint pElems\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\tpub = noStarchSoup.select('time')\n",
    "\t#pub = noStarchSoup.find('span', class_ = 'aQJ') # date\n",
    "\t#print pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h1')#, class_ = 'heading')\n",
    "\t\n",
    "\toregon = open('ORgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\toregon.write(re.split(\"<\", re.split('>', str(title))[1])[0])\n",
    "\t\t\t\toregon.write('\\n')\n",
    "\t\t\t\toregon.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\toregon.write('\\n')\n",
    "\t\t\t\t#try:\n",
    "\t\t\t\t#\toregon.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t#\toregon.write('\\n')\n",
    "\t\t\t\t#except:\n",
    "\t\t\t\t#\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\t#if re.search('###', str(pElems[i])):\n",
    "\t\t#\toregon.write(\"###\")\n",
    "\t\t#\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\toregon.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\toregon.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "North Carolina releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.nc.gop/news?page=3)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 6): #6\n",
    "\tx = \"https://www.nc.gop/news?page=\" + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\t#if re.search('http://nh.gop', str(link)) and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + [link['href']]\n",
    "\t\tif not re.search('http', str(link)) and len(str(link['href'])) > 17 and not re.search('twitter', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + ['https://www.nc.gop/news?page=' + str(i) + str(link['href'])]\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('NC_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/NC_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\tpub = noStarchSoup.select('time')\n",
    "\t#pub = noStarchSoup.find('span', class_ = 'aQJ') # date\n",
    "\tprint pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h1')#, class_ = 'heading')\n",
    "\tprint title\n",
    "\t#title = noStarchSoup.select('h2')\n",
    "\t#print re.split(\"</\", re.split('\">', str(title))[1])[0]\n",
    "\t#print title\n",
    "\t#print re.split('\\n', re.split('>\\n', str(title))[1])[0]\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t#print re.split('</time>', re.split('</span>', str(pub))[1])[0]\n",
    "\t\n",
    "\tnorthcarolina = open('NCgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tnorthcarolina.write(re.split(\"<\", re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tnorthcarolina.write('\\n')\n",
    "\t\t\t\tnorthcarolina.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\tnorthcarolina.write('\\n')\n",
    "\t\t\t\t#try:\n",
    "\t\t\t\t#\tnorthcarolina.write(re.split('</time>', re.split('</span>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t#\tnorthcarolina.write('\\n')\n",
    "\t\t\t\t#except:\n",
    "\t\t\t\t#\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tnorthcarolina.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tnorthcarolina.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tnorthcarolina.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Hampshire releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://www.nh.gop/news/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 24): #24\n",
    "\tx = \"http://nh.gop/news-and-events/page/\" + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\t#try:\n",
    "\t#\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t#\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\t#except:\n",
    "\t#\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('http://nh.gop', str(link)) and not re.search('#respond', str(link)) and len(str(link['href'])) > 40:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\t\t#if not re.search('http', str(link)) and len(str(link['href'])) > 10 and not re.search('twitter', str(link)):\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + ['http://www.massgop.com/' + str(link['href'])]\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('NH_gop')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/NH_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('span', class_ = 'aQJ') # date\n",
    "\tprint pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h1', class_ = 'heading')\n",
    "\t#title = noStarchSoup.select('h2')\n",
    "\t#print re.split(\"</\", re.split('\">', str(title))[1])[0]\n",
    "\tprint title\n",
    "\n",
    "\t\n",
    "\tnewhampshire = open('NHgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tnewhampshire.write(re.split(\"</\", re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tnewhampshire.write('\\n')\n",
    "\t\t\t\tnewhampshire.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tnewhampshire.write(re.split('\\n', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t\tnewhampshire.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tnewhampshire.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tnewhampshire.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tnewhampshire.write(\"\\n\")\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nebraska releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://www.ne.gop/news/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 2): \n",
    "\tx = \"http://www.ne.gop/news/\"# + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('http://www.ne.gop/201', str(link)): #, str(link)) and not re.search('#respond', str(link)) and len(str(link['href'])) > 48:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('NE_gop')\n",
    "os.chdir('C:/Users/Steve/Desktop/State_Party_PR/NE_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#res = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('h1', class_ = 'entry-title') # date\n",
    "\t#print pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h1', class_ = 'entry-title')\n",
    "\t#title = noStarchSoup.select('h2')\n",
    "\t#print re.split(\"</\", re.split('\">', str(title))[1])[0]\n",
    "\tprint title\n",
    "\t#print re.split('\\n', re.split('>\\n', str(title))[1])[0]\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tnebraska = open('NEgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tnebraska.write(re.split(\"</\", re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tnebraska.write('\\n')\n",
    "\t\t\t\tnebraska.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tnebraska.write(re.split('\\n', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t\tnebraska.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('Get all the latest news and updates from the Nebraska Republican Party', str(pElems[i])):\n",
    "\t\t\tnebraska.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('Get all the latest news and updates from the Nebraska Republican Party', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tnebraska.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tnebraska.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montana releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://mtgop.org/category/news/page/2/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 12): #12\n",
    "\tx = \"http://mtgop.org/category/news/page/\" + str(i) #+ \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\t#try:\n",
    "\t#\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t#\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\t#except:\n",
    "\t#\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('mtgop.org', str(link)) and not re.search('#respond', str(link)) and len(str(link['href'])) > 48:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\t\t#if not re.search('http', str(link)) and len(str(link['href'])) > 10 and not re.search('twitter', str(link)):\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + ['http://www.massgop.com/' + str(link['href'])]\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('MT_gop')\n",
    "os.chdir('C:/Users/Steve/Desktop/State_Party_PR/MT_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('h1', class_ = 'title') # date\n",
    "\t#print pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.find('h1', class_ = 'title')\n",
    "\t#title = noStarchSoup.select('h2')\n",
    "\t#print re.split(\"</\", re.split('\">', str(title))[1])[0]\n",
    "\tprint title\n",
    "\t#print re.split('\\n', re.split('>\\n', str(title))[1])[0]\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tmontana = open('MTgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tmontana.write(re.split(\"</\", re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tmontana.write('\\n')\n",
    "\t\t\t\tmontana.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmontana.write(re.split('\\n', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t\tmontana.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tmontana.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tmontana.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tmontana.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massachusetts releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://www.massgop.com/press?page=2)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 31): #31\n",
    "\tx = \"http://www.massgop.com/press?page=\" + str(i) #+ \"/\"\n",
    "\t#resp = urllib2.urlopen(x)\n",
    "\ttry:\n",
    "\t\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\t#if re.search('massgop.com', str(link)) and not re.search('#comments', str(link)):\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + [link['href']]\n",
    "\t\tif not re.search('http', str(link)) and len(str(link['href'])) > 10 and not re.search('twitter', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + ['http://www.massgop.com/' + str(link['href'])]\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('MA_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('div', class_ = 'byline')\n",
    "\t#print pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.select('h2')\n",
    "\tprint title\n",
    "\t#print re.split('\\n', re.split('>\\n', str(title))[1])[0]\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tmassachusetts = open('MAgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tmassachusetts.write(re.split(\"</\", re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tmassachusetts.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmassachusetts.write(re.split('\\n', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t\tmassachusetts.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tmassachusetts.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tmassachusetts.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tmassachusetts.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pennsylvania releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.pagop.org/category/pressreleases/page/13/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 107): \n",
    "\tx = \"https://www.pagop.org/category/pressreleases/page/\" + str(i) + \"/\"\n",
    "\tresp = urllib2.urlopen(x)\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('https://www.pagop.org/201', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('pennsylvaniaGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/alaskaGOP_press_releases')\n",
    "for link in links:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\tpub = noStarchSoup.select('time')\n",
    "\t#title = noStarchSoup.select('h1')\n",
    "\ttitle = noStarchSoup.find('h1', attrs={'class': 'entry-title'})\n",
    "\tprint str(pub)\n",
    "\tprint str(title)\n",
    "\t#pennsylvania = open('PA_' + link[30:-1] + '.txt', 'w')\n",
    "\tpennsylvania = open('PAgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint str(pub)\n",
    "\t\t\t\tprint str(title)\n",
    "\t\t\t\tpennsylvania.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tpennsylvania.write('\\n')\n",
    "\t\t\t\tpennsylvania.write(re.split('<', re.split('>', str(pub))[1])[0])\n",
    "\t\t\t\tpennsylvania.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tpennsylvania.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tpennsylvania.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tpennsylvania.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Jersey releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://www.njgop.org/news/page/13/)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 14): \n",
    "\tx = \"http://www.njgop.org/news/page/\" + str(i) + \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\tif re.search('http://www.njgop.org/201', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + [link['href']]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('newjerseyGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/alaskaGOP_press_releases')\n",
    "for link in links:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\tprint str(pub)\n",
    "\t#print str(title)\n",
    "\t#newjersey = open('NJgop_' + link[32:-1] + '.txt', 'w')\n",
    "\tnewjersey = open('NJgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tnewjersey.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tnewjersey.write('\\n')\n",
    "\t\t\t\tnewjersey.write(re.split('</', re.split('>', str(pub))[1])[0])\n",
    "\t\t\t\tnewjersey.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tnewjersey.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tnewjersey.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tnewjersey.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Florida releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (http://www.florida.gop/press_releases?page=2)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 11): #11\n",
    "\tx = \"http://www.florida.gop/press_releases?page=\" + str(i) #+ \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', class_ = 'btn btn-primary btn-sm btn-square marginbottomless'):  #soup.find_all('a', href=True)\n",
    "\t\tif re.search('', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + ['http://www.florida.gop' + str(link['href'])] \n",
    "\t\t\tprint links\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('floridaGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/floridaGOP_press_releases')\n",
    "for link in links:\n",
    "\tres = ''\n",
    "\twhile res=='':\n",
    "\t\ttry:\n",
    "\t\t\tres = requests.get(link)\n",
    "\t\texcept:\n",
    "\t\t\tbreak\n",
    "#\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t\t\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\tpub = noStarchSoup.select('time')\n",
    "\t#pub = noStarchSoup.find('time', attrs={'class': 'entry-date published updated'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\tprint str(pub)\n",
    "\t#print str(title)\n",
    "\t#florida = open(re.sub('/','_',('FLgop_' + link[22:-1] + '.txt')), 'w')\n",
    "\tflorida = open('FLgop_' + str(counter) + '.txt', 'w')\n",
    "\t#print pElems\n",
    "\tcounter += 1\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint str(pub)\n",
    "\t\t\t\tprint str(title)\n",
    "\t\t\t\tflorida.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tflorida.write('\\n')\n",
    "\t\t\t\tflorida.write(re.split('H', re.split('datetime=\"', str(pub))[1])[0])\n",
    "\t\t\t\tflorida.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search(\"Optional email code\", str(pElems[i])):\n",
    "\t\t\tflorida.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tflorida.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tflorida.write(\"\\n\")\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.cagop.org/news?page=2)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 7): #7\n",
    "\tx = \"https://www.cagop.org/news?page=\" + str(i) #+ \"/\"\n",
    "\ttry:\n",
    "\t\tresp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', title = 'Read More'):  #soup.find_all('a', href=True)\n",
    "\t\tif re.search('', str(link)):\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + ['https://www.cagop.org' + str(link['href'])]\n",
    "\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('californiaGOP_press_releases')\n",
    "#os.chdir('C:/Users/Steve/Desktop/State_Party_PR/scripts/alaskaGOP_press_releases')\n",
    "for link in links:\n",
    "\ttry:\n",
    "\t\tres = requests.get(link)\n",
    "\texcept:\n",
    "\t\tbreak\n",
    "#\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t\t\n",
    "\t#print type(noStarchSoup)\n",
    "\tpElems = noStarchSoup.select('p')\n",
    "\tpub = noStarchSoup.select('time')\n",
    "\t#pub = noStarchSoup.find('time', attrs={'class': 'entry-date published updated'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\tprint str(pub)\n",
    "\t#print str(title)\n",
    "\t#california = open(re.sub('/','_',('CAgop_' + link[22:-1] + '.txt')), 'w')\n",
    "\tcalifornia = open('CAgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint str(pub)\n",
    "\t\t\t\tprint str(title)\n",
    "\t\t\t\tcalifornia.write(re.split('<', re.split('>', str(title))[1])[0])\n",
    "\t\t\t\tcalifornia.write('\\n')\n",
    "\t\t\t\tcalifornia.write(re.split('H', re.split('datetime=\"', str(pub))[1])[0])\n",
    "\t\t\t\tcalifornia.write('\\n')\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search(\"Today, it's as important as ever to stand up and make sure you're a part of the solution for\", str(pElems[i])):\n",
    "\t\t\tcalifornia.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tcalifornia.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tcalifornia.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maryland releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store press release urls\n",
    "links = []\n",
    "\t\t\n",
    "# Iterate through pages of press release links\n",
    "# The url for each page follows a set pattern (https://www.mdgop.org/news?page=2)\n",
    "# therefore each iteration changes the page of press release links\n",
    "# Appends all press releases URLs (ending with .../newsroom/\"some digit\") to a list\n",
    "for i in range(1, 14): #24 \n",
    "\tx = \"https://www.mdgop.org/news?page=\" + str(i) #+ \"/\"\n",
    "\t#resp = urllib2.urlopen(x)\n",
    "\ttry:\n",
    "\t\treq = urllib2.Request(x, headers={'User-Agent' : \"Magic Browser\"})\n",
    "\t\tresp = urllib2.urlopen( req )\n",
    "\t\t#resp = urllib2.urlopen(x)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tsoup = BeautifulSoup(resp, from_encoding=resp.info().getparam('charset'))\n",
    "\tfor link in soup.find_all('a', href=True):\n",
    "\t\t#if re.search('www.mdgop.org', str(link)) and not re.search('#comments', str(link)):\n",
    "\t\t#\tprint link['href']\n",
    "\t\t#\tlinks = links + [link['href']]\n",
    "\t\tif not re.search('http', str(link)) and len(str(link['href'])) > 1:\n",
    "\t\t\tprint link['href']\n",
    "\t\t\tlinks = links + ['https://www.mdgop.org/' + str(link['href'])]\n",
    "\t\t\n",
    "links = list(set(links))\n",
    "print len(links)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "#os.makedirs('MD_gop')\n",
    "for link in links:\n",
    "\t#if not re.search('http:', str(link)):\n",
    "\t#\tres = requests.get(str('http:' + str(link)))\n",
    "\t#else:\n",
    "\tres = requests.get(link)\n",
    "\t#res.raise_for_status()\n",
    "\tnoStarchSoup = bs4.BeautifulSoup(res.text, 'html5lib')\n",
    "\t#print type(noStarchSoup)\n",
    "\t#pElems = noStarchSoup.select('paragraph')\n",
    "\tpElems = noStarchSoup.find_all('p')\n",
    "\t#pElems = noStarchSoup.find_all('div', class_ = 'paragraph')\n",
    "\t#pub = noStarchSoup.select('time')\n",
    "\tpub = noStarchSoup.find('div', class_ = 'byline')\n",
    "\t#print pub\n",
    "\t#print re.split('\\n', re.split('</i>', str(pub))[1])[0].strip()\n",
    "\t#pub = noStarchSoup.find('small', attrs={'class': 'date'})  \n",
    "\ttitle = noStarchSoup.select('h1')\n",
    "\tprint title\n",
    "\t#print re.split('\\n', re.split('>\\n', str(title))[1])[0]\n",
    "\t#title = noStarchSoup.find('h2', class_ = 'blog-title')\n",
    "\t#title = noStarchSoup.find('h1', attrs={'class': 'col-md-8 col-md-offset-2'})\n",
    "\t#print str(pub)\n",
    "\t#print title\n",
    "\t#print re.split('</', re.split('text\">', str(pub))[1])[0].strip()\n",
    "\t#print str(title)\n",
    "\t\n",
    "\tmaryland = open('MDgop_' + str(counter) + '.txt', 'w')\n",
    "\tcounter += 1\n",
    "\t#print pElems\n",
    "\tfor i in range(len(pElems)):\n",
    "\t\t# Write parsed date to beginning of .txt   \n",
    "\t\tif i == 0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#print str(pub)\n",
    "\t\t\t\t#print str(title)\n",
    "\t\t\t\tmaryland.write(re.split(\"</\", re.split('\">', str(title))[1])[0])\n",
    "\t\t\t\tmaryland.write('\\n')\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmaryland.write(re.split('\\n', re.split('</i>', str(pub))[1])[0].strip())\n",
    "\t\t\t\t\tmaryland.write('\\n')\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t# When a paragraph with \"###\" is reached, the for loop breaks and the next url is parsed\n",
    "\t\t# This ensures that comments under the press release are not included in .txt\n",
    "\t\tif re.search('###', str(pElems[i])):\n",
    "\t\t\tmaryland.write(\"###\")\n",
    "\t\t\tbreak\n",
    "\t\t# For all paragraphs that do not contain \"###\" (used to indicate end of PR), write to .txt\n",
    "\t\tif not re.search('###', str(pElems[i])):\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\t#print pElems[i].getText().encode(\"utf-8\")\n",
    "\t\t\tmaryland.write(pElems[i].getText().encode(\"utf-8\"))\n",
    "\t\t\tmaryland.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
